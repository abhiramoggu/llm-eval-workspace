# Experiments & Outputs (auto-generated by the pipeline)

This repo produces **three categories** of results:

## A) Core grounded TAS evaluation (per conversation)
Computed in `evaluate.py`:
- **TAS** (Trajectory Adaptation Score) and its components:
  - CO: Concept Overlap
  - WCS: Weighted Constraint Similarity
  - CP: Copying Penalty (lower is better; radar plots use 1-CP for readability)
- **Shift recovery diagnostics**
  - RR: Topic Recovery Rate
  - RD: Average Recovery Delay (in turns)
  - Both are computed for **Jaccard-based** and **Cosine-based** recovery variants.
- **Recommendation-grounded proxy**
  - `recommendation_satisfaction_mean`: how well recommended items satisfy the expressed constraints (no user history needed).
- **Baselines**
  - `semantic_similarity_mean`: non-grounded semantic similarity between USER and SYSTEM turns.
  - `dst_*`: DST-style set precision/recall/F1 and joint accuracy against simulator “gold” constraints (when available).

## B) LLM-as-a-Judge rubric (per conversation)
Computed in `evaluate.py` via `llm_judge()`:
- proactiveness
- coherence
- personalization
(and any additional rubric fields configured)

## C) Analysis/visualization outputs
Generated by `analyze_results.py`:
- Tables: `figures/tables/model_metrics.csv`, `recovery_metrics.csv`, `spearman_correlation.csv`, `tas_volatility.csv`
- Plots:
  - TAS vs baselines
  - TAS components bar + **TAS-only radar** (no CAS radar)
  - RR/RD plots (Jaccard vs Cosine)
  - Judge stacked normalized (0–1)
  - Distributions, scatter plots, correlations heatmap
  - TAS over turns + TAS volatility
  - Optional 3D visualization (turn × component × value)

## Sensitivity analysis
Run `python sensitivity_analysis.py` to sweep TAS weights and produce heatmaps under `figures/sensitivity/`.

## Concept inspection
Run `python inspect_concepts.py` to dump the most frequent extracted concepts to CSV under `figures/concept_inspection/`.
